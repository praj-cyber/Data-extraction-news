{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery import Celery\n",
    "import feedparser\n",
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set up Celery app\n",
    "app = Celery('tasks', broker='amqp://guest@localhost//')\n",
    "\n",
    "# Set up database connection\n",
    "engine = create_engine('postgresql://user:password@host:port/dbname')\n",
    "Base = declarative_base()\n",
    "\n",
    "class Article(Base):\n",
    "    __tablename__ = 'articles'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    title = Column(String)\n",
    "    content = Column(String)\n",
    "    publication_date = Column(DateTime)\n",
    "    source_url = Column(String)\n",
    "    category = Column(String)\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='app.log', level=logging.INFO)\n",
    "\n",
    "def log_event(event):\n",
    "    logging.info(event)\n",
    "\n",
    "def handle_error(error):\n",
    "    logging.error(error)\n",
    "\n",
    "# Load NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define categories\n",
    "categories = ['Terrorism / protest / political unrest / riot',\n",
    "               'Positive/Uplifting',\n",
    "               'Natural Disasters',\n",
    "               'Others']\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "def train_classifier():\n",
    "    train_data = []\n",
    "    for category in categories:\n",
    "        with open(f'{category}.txt', 'r') as f:\n",
    "            texts = f.readlines()\n",
    "            for text in texts:\n",
    "                tokens = word_tokenize(text)\n",
    "                train_data.append((dict([(word, True) for word in tokens]), category))\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "    return classifier\n",
    "\n",
    "classifier = train_classifier()\n",
    "\n",
    "# Define task to process article\n",
    "@app.task\n",
    "def process_article(article):\n",
    "    try:\n",
    "        # Category classification using NLTK\n",
    "        category = classify_article(article['content'])\n",
    "        article['category'] = category\n",
    "        store_article(article)\n",
    "        log_event(f'Article {article[\"title\"]} processed successfully')\n",
    "    except Exception as e:\n",
    "        handle_error(f'Error processing article {article[\"title\"]}: {str(e)}')\n",
    "\n",
    "def classify_article(content):\n",
    "    tokens = word_tokenize(content)\n",
    "    features = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        features[token] += 1\n",
    "    return classifier.classify(dict(features))\n",
    "\n",
    "def store_article(article):\n",
    "    existing_article = session.query(Article).filter_by(title=article['title']).first()\n",
    "    if existing_article is None:\n",
    "        new_article = Article(**article)\n",
    "        session.add(new_article)\n",
    "        session.commit()\n",
    "\n",
    "# Define task to parse feeds and extract articles\n",
    "@app.task\n",
    "def parse_feeds(feeds):\n",
    "    articles = []\n",
    "    for feed in feeds:\n",
    "        parsed_feed = feedparser.parse(feed)\n",
    "        for entry in parsed_feed.entries:\n",
    "            article = {\n",
    "                'title': entry.title,\n",
    "                'content': entry.summary,\n",
    "                'publication_date': entry.published,\n",
    "                'source_url': entry.link\n",
    "            }\n",
    "            articles.append(article)\n",
    "    return articles\n",
    "\n",
    "# Define task to send articles to task queue\n",
    "@app.task\n",
    "def send_articles_to_queue(articles):\n",
    "    for article in articles:\n",
    "        process_article.delay(article)\n",
    "\n",
    "# Run the application\n",
    "if __name__ == '__main__':\n",
    "    feeds = [\n",
    "        'http://rss.cnn.com/rss/cnn_topstories.rss',\n",
    "        'http://qz.com/feed',\n",
    "        'http://feeds.foxnews.com/foxnews/politics',\n",
    "        'http://feeds.reuters.com/reuters/businessNews',\n",
    "        'http://feeds.feedburner.com/NewshourWorld',\n",
    "        'https://feeds.bbci.co.uk/news/world/asia/india/rss.xml'\n",
    "    ]\n",
    "    articles = parse_feeds(feeds)\n",
    "    send_articles_to_queue(articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
